{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec902870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8e2988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_opt.csv\")\n",
    "df = df[df['season'] != 2013]\n",
    "\n",
    "features = ['home_win', 'away_win', 'pi_diff','gd_diff', \n",
    "            'elo_gls_diff', 'home_ip_scaled', 'away_ip_scaled',\n",
    "       'home_ip', 'away_ip', 'ip_diff', 'season']\n",
    "\n",
    "df = df[features].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66d5a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimiser:\n",
    "    \n",
    "    def __init__(self, df, model_name, verbosity=False):\n",
    "        self.df = df\n",
    "        self.model_name = model_name\n",
    "        self.verbosity = verbosity\n",
    "        \n",
    "        if self.model_name == \"XGB\":\n",
    "            self.model = XGBClassifier()\n",
    "        elif self.model_name == \"LGBM\":\n",
    "            self.model = LGBMClassifier()\n",
    "        elif self.model_name == \"GBC\":\n",
    "            self.model = GradientBoostingClassifier()\n",
    "        elif self.model_name == \"CAT\":\n",
    "            self.model = CatBoostClassifier(verbose = verbosity)\n",
    "            \n",
    "    def process_df(self, df):\n",
    "        \n",
    "        df = df.astype(float)\n",
    "        df = df[df['season'] != 2013]\n",
    "        df = df.dropna()\n",
    "        train = df[df['season'] != 2022.]\n",
    "        test = df[df['season'] == 2022.]\n",
    "        \n",
    "        X_train = train.drop(['home_win', 'away_win', 'season'], axis=1)\n",
    "        X_test = test.drop(['home_win', 'away_win', 'season'], axis=1)\n",
    "        y_train = train['home_win']\n",
    "        y_test = test['home_win']\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def objective(self, trial):\n",
    "\n",
    "\n",
    "        if self.model_name == \"XGB\":\n",
    "            params = {\n",
    "                \n",
    "            'booster': 'gbtree',\n",
    "            'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
    "            'scale_pos_weight': (0.45/0.55),\n",
    "            'subsample': trial.suggest_float('subsample', 0.2, 1.0, step=0.1),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0, step=0.1),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-5, 10, log=True),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-5, 10, log=True),\n",
    "            'gamma': trial.suggest_float('gamma', 1e-5, 10, log=True),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10)\n",
    "                \n",
    "        }\n",
    "            model = XGBClassifier(**params)\n",
    "\n",
    "        elif self.model_name == \"LGBM\":\n",
    "            \n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
    "                'subsample': trial.suggest_float('subsample', 0.2, 1.0, step=0.1),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0, step=0.1),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-5, 10, log=True),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-5, 10, log=True),\n",
    "                'min_child_weight': trial.suggest_float('min_child_weight', 1e-5, 10, log=True)\n",
    "            }\n",
    "\n",
    "            model = LGBMClassifier(**params)\n",
    "            \n",
    "        elif self.model_name == \"GBC\":\n",
    "            \n",
    "            params = {\n",
    "                'loss': 'deviance',\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
    "                'subsample': trial.suggest_float('subsample', 0.2, 1.0, step=0.1),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "                'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n",
    "                'random_state': 42\n",
    "            }\n",
    "\n",
    "            model = GradientBoostingClassifier(**params)\n",
    "            \n",
    "        elif self.model_name == \"CAT\":\n",
    "\n",
    "            params = {\n",
    "                'iterations': trial.suggest_int('iterations', 100, 1000, step=100),\n",
    "                'depth': trial.suggest_int('depth', 1, 10),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "                'random_seed': 42,\n",
    "                'loss_function': 'Logloss',\n",
    "                'eval_metric': 'Accuracy',\n",
    "                'bootstrap_type': 'Bayesian',\n",
    "                'subsample': trial.suggest_float('subsample', 0.2, 1.0, step=0.1),\n",
    "                'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.2, 1.0, step=0.1),\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-5, 10, log=True),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 1, 20),\n",
    "                'verbose': self.verbosity\n",
    "            }\n",
    "\n",
    "            model = CatBoostClassifier(**params)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = self.process_df(self.df)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_prob = model.predict_proba(X_test)\n",
    "        auc = roc_auc_score(y_test, y_prob[:, 1])\n",
    "\n",
    "        return auc\n",
    "\n",
    "    def optimise(self):\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self.process_df(self.df)\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(self.objective, n_trials=100, show_progress_bar=self.verbosity)\n",
    "        best_params = study.best_params\n",
    "        \n",
    "        if self.model_name == \"XGB\":\n",
    "            best_model = XGBClassifier(**best_params)\n",
    "            \n",
    "        elif  self.model_name == \"LGBM\":\n",
    "            best_model = LGBMClassifier(**best_params)\n",
    "        \n",
    "        elif  self.model_name == \"GBC\":\n",
    "            best_model = GradientBoostingClassifier(**best_params)\n",
    "            \n",
    "        elif  self.model_name == \"CAT\":\n",
    "            best_model = CatBoostClassifier(**best_params)\n",
    "            \n",
    "        kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "        results = cross_val_score(self.model, self.X_train, self.y_train, cv=kfold, verbose=self.verbosity)\n",
    "        \n",
    "        print(\"Results of K-fold CV for\", self.model, \":\", results)\n",
    "        print(\"Mean:\", results.mean())\n",
    "        print(\"Std:\", results.std())\n",
    "\n",
    "        best_model.fit(self.X_train, self.y_train, \n",
    "                       early_stopping_rounds=1000, \n",
    "                       eval_set=[(self.X_test, self.y_test)],\n",
    "                       verbose=self.verbosity)\n",
    "\n",
    "        y_pred_test = best_model.predict(self.X_test)\n",
    "        y_prob_test = best_model.predict_proba(self.X_test)[:, 1]\n",
    "\n",
    "        test_accuracy = accuracy_score(self.y_test, y_pred_test)\n",
    "        test_f1 = f1_score(self.y_test, y_pred_test)\n",
    "\n",
    "        test_roc_auc_score = roc_auc_score(self.y_test, y_prob_test)\n",
    "        test_precision_score = precision_score(self.y_test, y_pred_test)\n",
    "\n",
    "        print(\"Best Hyperparameters:\", best_params)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "        print(\"Test F1 Score:\", test_f1)\n",
    "        print(\"Test AUC:\", test_roc_auc_score)\n",
    "        print(\"Test Precision Score:\", test_precision_score)\n",
    "\n",
    "        return best_model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
